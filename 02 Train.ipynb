{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets.RNA_graph_classification.load_net import gnn_model # import all GNN from data.data \n",
    "import LoadData # import dataset\n",
    "%matplotlib inline\n",
    "import dgl\n",
    "import torch\n",
    "import matplotlib.pyplot as plt import networkx as nx\n",
    "from dgl.nn.pytorch import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import json\n",
    "\n",
    "arg_config = \"./configs/RNAgraph_graph_classification_GCN_in_vivo_100k.json\"\n",
    "\n",
    "with open(arg_config) as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"ALKBH5_Baltz2012\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "dataset = LoadData(DATASET_NAME, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs # (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes): super(Classifier, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim) \n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim) \n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "        \n",
    "    def forward(self, g):\n",
    "        # Use node degree as the initial node feature. For undirected graphs, # is the same as the out_degree.\n",
    "        h = g.in_degrees().view(-1, 1).float()\n",
    "        # Perform graph convolution and activation function.\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        g.ndata['h'] = h\n",
    "        # Calculate graph representation by averaging all the node representa hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create training and test sets.\n",
    "trainset = dataset.train\n",
    "testset = dataset.test\n",
    "\n",
    "# Use PyTorch's DataLoader and the collate function\n",
    "# defined before.\n",
    "data_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n",
    "collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Classifier(1, 256, 2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) model.train()\n",
    "\n",
    "# Model\n",
    "epoch_losses = []\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    for iter, (bg, label) in enumerate(data_loader):\n",
    "        prediction = model(bg)\n",
    "        loss = loss_func(prediction, label) optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss)) \n",
    "        epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('cross entropy averaged over minibatches') \n",
    "plt.plot(epoch_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
